{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "super-suspension",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between; align-items: center; padding: 10 px;\">\n",
    "    <h3 style=\"margin: 0 0 20px; color: #FFFFFF; line-height: 1\"> CS5002: Programming Principles and Practice</h3>\n",
    "    <img src=\"../rsc/logo.png\" alt=\"University Logo\" style=\"width: 80px; height: auto;\"/>\n",
    "</div>\n",
    "                                  <!--  -->\n",
    "<div style=\"display: flex; justify-content: space-between; width: 100%;\">\n",
    "  <span style=\"text-align: left;\">Practical P3: Data analysis and visualisation Python</span>\n",
    "  <span style=\"text-align: right; font-size: 12px;\">Student ID: 200013825</span>\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d440feb",
   "metadata": {},
   "source": [
    "### Introduction \n",
    "This project aims to modularly read, clean and analyze real-world csv data. Below is the introduction on all the modules and major functions that worked together to complete data analysis.\n",
    "\n",
    "`data_mining.py`: This executable python \n",
    "script automates data cleaning process, I used \n",
    "```python\n",
    "invalid_mask = ~data[column].isin(valid_set)\n",
    "```\n",
    "to collect a list of booleans, indicating whether each csv code is admissible. \n",
    "\n",
    "Based on the context of this dataset, it is more efficient to neglect the type of the data when checking consistency, and then convert all data into pandas' categorical type. \n",
    "``` python\n",
    "data[column] = data[column].astype(str)\n",
    "```\n",
    "I have converted all column in the data as string to help the determination of admissible values, and checked for missing columns or values, so then if encountered with any missing value, the row will be dropped.\n",
    "\n",
    "To decode the csv data and to give context to the later stages of data analysis, i incorprated the data mapping at the stage of data_mining, so that the **cleaned_data.csv** contains all the code derived from json, with all the data being cleaned. I achieved this step by creating a helper function called **map_data** which takes the data to map as well as the json that was loaded in the **clean_data** function, and use **.map()** to map the data based on json if the column name match wth the json dictionary keys. Below is the logic to map data\n",
    "```python\n",
    "for column in data_to_map.columns:\n",
    "        if column in decode_helper:\n",
    "            data_to_map[column] = data_to_map[column].map(decode_helper[column])\n",
    "```\n",
    "\n",
    "`data_analysis.py`: This module contains function **analyze_data** and compute all the required analysis, then store all output from different requirements as a big dictionaries of dictionaires. This makes the overall workflow modular and clean with minimal repetition. One drawback would be the fact that all the analysis is hard-coded in, and reusability is diminished from this specific feature.\n",
    "\n",
    "There are in total of 12 required data analysis, in the **data_analysis.py**, i have set them as requirements 1-12, and used pandas to analyze. A few analysis will be illustrated below. \n",
    "\n",
    "1. To get the percentage of records for any column, i used:\n",
    "```python\n",
    "column_counts = data['col_name'].value_counts(normalize=True) * 100\n",
    "# then append the result to the big collective dictionary\n",
    "results['requirement n name'] = column_counts.round(2).to_dict()\n",
    "```\n",
    "2. To group two columns together, i used:\n",
    "```python\n",
    "results['requirement n name'] = (data.groupby(['col_1', 'col_2']).size().to_dict())\n",
    "```\n",
    "As shown above, i collect each reqired results and have stored them in one dictionary called results. This helps with further processing and storing, as dictionary can centralize calculated results, and it can store a diverse range of data types.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4448bcd7",
   "metadata": {},
   "source": [
    "### Problem Encountered\n",
    "\n",
    "##### __1. infinite commit loop__: \n",
    "When I was trying to set up hooks for git to automatically update the gitlog everytime after commit, i accidently created an infinite loop of git commit, below was what I have included in the post-commit* file in .git/hooks:\n",
    "\n",
    "``` python\n",
    "#!/bin/bash\n",
    "git log --oneline  --graph  > gitlog.txt\n",
    "git commit -m 'gitlog.txt is updated with the latest commit history'\n",
    "```\n",
    "and this created redundant git log histroy that are shown in the gitlog.txt\n",
    "\n",
    "I fixed it by deleting the line to commit within post-commit, after i realized that adding commit within post-commit can trigger recursive calls.\n",
    "\n",
    "##### __2. system path append__: \n",
    "When I was trying to append certain relavant path to the sys path in order for jupyter notebook to read the file, i added the file path to sys path. After I print sys.path to check, i saw multiple paths being appended in the system paths, in order to clean this, i imported `site` which is a package that can help clear all custom paths and only keep the default path. I then used \n",
    "```python\n",
    "sys.path = list(site.getsitepackages()) + sys.path [:1]\n",
    "```\n",
    "to implement this strategy. \n",
    "After clearing the system path, i appended the relative path using sys.path.append, and successfully imported the modules from **code** folder. As shown below.\n",
    "```python\n",
    "sys.path.append('./code')\n",
    "```\n",
    "Certain compiled python file was created to help faster import, and such files were added to .gitignore to prevent unncessary commiting and pushing because **\\__pycache__\\** will be created everytime the imported modules are ran.\n",
    "\n",
    "##### __3. using venv in jupyter notebook__: \n",
    "\n",
    "\n",
    "##### __4. html header editing__: \n",
    "When i was trying to create proffesional looking header using html, it worked perfectly in vscode like so:\n",
    "***\n",
    "![Image](../rsc/Jupyter_heading.png)\n",
    "***\n",
    "But when i viewed it in jupyter notebook via jupyter, it did not look that aligned as shown below: \n",
    "***\n",
    "![Image](../rsc/Jupyter_heading_notgood.png)\n",
    "***\n",
    "\n",
    "\n",
    "|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7071106c",
   "metadata": {},
   "source": [
    "**First set up the neccessary dependencies for the project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-hormone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been analyzed and a dictionary with \n",
      " dict_keys(['total_record', 'data_types', 'value_counts', 'age_count', 'occupation_counts', 'health_percentages', 'ethnic_group_percentages', 'hours_worked_by_industry', 'occupation_by_social_grade', 'economically_active_by_age', 'economically_inactive_by_health', 'working_hours_for_students']) \n",
      " columns have been created ans stored! \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../code')\n",
    "\n",
    "from data_analysis import analyze_csv\n",
    "\n",
    "# results_dict is the dictionary that the analyze_data function returns\n",
    "resutlts_dict = analyze_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40da94f6",
   "metadata": {},
   "source": [
    "#### Basic Requirement\n",
    "**Question 1: the total number of records in the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94dec630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63388"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resutlts_dict['total_record']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545f51cc",
   "metadata": {},
   "source": [
    "**Question 2: the type of each variable in the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db8171f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'record_number': dtype('int64'),\n",
       " 'region': dtype('O'),\n",
       " 'residence_type': dtype('O'),\n",
       " 'family_composition': dtype('O'),\n",
       " 'sex': dtype('O'),\n",
       " 'age': dtype('O'),\n",
       " 'marital_status': dtype('O'),\n",
       " 'student': dtype('O'),\n",
       " 'country_of_birth': dtype('O'),\n",
       " 'health': dtype('O'),\n",
       " 'ethnic_group': dtype('O'),\n",
       " 'religion': dtype('O'),\n",
       " 'economic_activity': dtype('O'),\n",
       " 'occupation': dtype('O'),\n",
       " 'industry': dtype('O'),\n",
       " 'hours_worked_per_week': dtype('O'),\n",
       " 'approximate_social_grade': dtype('O')}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resutlts_dict['data_types']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e26a0f",
   "metadata": {},
   "source": [
    "**Question 3: the total number of records in the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "951704d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Very good health': 52.75,\n",
       " 'Good health': 29.7,\n",
       " 'Fair health': 11.9,\n",
       " 'Bad health': 4.35,\n",
       " 'Very bad health': 1.3}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resutlts_dict['health_percentages']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84114694",
   "metadata": {},
   "source": [
    "**Question 4: the total number of records in the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de93797",
   "metadata": {},
   "source": [
    "**Question 5: the total number of records in the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0a5e8d",
   "metadata": {},
   "source": [
    "**Question 6: the total number of records in the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e05e1bf",
   "metadata": {},
   "source": [
    "**Question 3: the total number of records in the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7968baa6",
   "metadata": {},
   "source": [
    "**Question 3: the total number of records in the dataset**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1480b5c5",
   "metadata": {},
   "source": [
    "**Question 3: the total number of records in the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f5bc1e",
   "metadata": {},
   "source": [
    "**Question 3: the total number of records in the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfadc7ee",
   "metadata": {},
   "source": [
    "**Question 3: the total number of records in the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b196ce",
   "metadata": {},
   "source": [
    "**Question 3: the total number of records in the dataset**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python(venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
