{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "super-suspension",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"display: flex; justify-content: space-between; align-items: center; padding: 10 px;\">\n",
    "    <h3 style=\"margin: 0 0 20px; color: #FFFFFF; line-height: 1\">CS5002: Programming Principles and Practice</h3>\n",
    "    <img src=\"../rsc/logo.png\" alt=\"University Logo\" style=\"width: 80px; height: auto;\"/>\n",
    "</div>\n",
    "                                  <!--  -->\n",
    "<div style=\"display: flex; justify-content: space-between; width: 100%;\">\n",
    "  <span style=\"text-align: left;\">Practical P3: Data analysis and visualisation Python</span>\n",
    "  <span style=\"text-align: right; font-size: 12px;\">Student ID: 200013825</span>\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d440feb",
   "metadata": {},
   "source": [
    "#### Introduction \n",
    "This project aims to modularly read, clean and analyze real-world csv data. Below is the introduction on all the modules and major functions that worked together to complete data analysis.\n",
    "\n",
    "`data_mining.py`: This executable python \n",
    "script automates data cleaning process, I used \n",
    "```python\n",
    "invalid_mask = ~data[column].isin(valid_set)\n",
    "```\n",
    "to collect a list of booleans, indicating whether each csv code is admissible. \n",
    "\n",
    "Based on the context of this dataset, it is more efficient to neglect the type of the data when checking consistency, and then convert all data into pandas' categorical type. \n",
    "``` python\n",
    "data[column] = data[column].astype(str)\n",
    "```\n",
    "I have converted all column in the data as string to help the determination of admissible values, and implemented the strategy of filling the missing column with \"Unknown\" to act as a place holder\n",
    "\n",
    "I have also checked for missing columns and \n",
    "\n",
    "`data_analysis.py`: This module contains function **analyze_data** and compute all the required analysis, then store all output from different requirements as a big dictionaries of dictionaires. This makes the overall workflow modular and clean with minimal repetition. One drawback would be the fact that all the analysis is hard-coded in, and reusability is diminished from this specific feature. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4448bcd7",
   "metadata": {},
   "source": [
    "#### Problem Encountered\n",
    "\n",
    "##### __1. infinite commit loop__: \n",
    "When I was trying to set up hooks for git to automatically update the gitlog everytime after commit, i accidently created an infinite loop of git commit, below was what I have included in the post-commit* file in .git/hooks:\n",
    "\n",
    "``` python\n",
    "#!/bin/bash\n",
    "git log --oneline  --graph  > gitlog.txt\n",
    "git commit -m 'gitlog.txt is updated with the latest commit history'\n",
    "```\n",
    "and this created redundant git log histroy that are shown in the gitlog.txt\n",
    "\n",
    "I fixed it by deleting the line to commit within post-commit, after i realized that adding commit within post-commit can trigger recursive calls.\n",
    "\n",
    "##### __1. system path append__: \n",
    "When I was trying to append certain relavant path to the sys path in order for jupyter notebook to read the file, i added the file path to sys path. After I print sys.path to check, i saw multiple paths being appended in the system paths, in order to clean this, i imported `site` which is a package that can help clear all custom paths and only keep the default path. I then used \n",
    "```python\n",
    "sys.path = list(site.getsitepackages()) + sys.path [:1]\n",
    "```\n",
    "to implement this strategy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7071106c",
   "metadata": {},
   "source": [
    "**First Import all the neccessary packages and modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-hormone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\n",
       "  File \"/Users/weltschmerz/.vscode/extensions/ms-python.python-2024.20.0-darwin-arm64/python_files/python_server.py\", line 130, in exec_user_input\n",
       "    retval = callable_(user_input, user_globals)\n",
       "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"<string>\", line 7, in <module>\n",
       "ImportError: cannot import name 'data_analysis' from 'code' (/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/code.py)\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('../code'))\n",
    "from data_analysis import analyze_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-keeping",
   "metadata": {},
   "source": [
    "We start with exploring the content of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/Scotland_teaching_file_1PCT.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-ethiopia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'pandas.core.frame.DataFrame'>\n",
       "RangeIndex: 63388 entries, 0 to 63387\n",
       "Data columns (total 17 columns):\n",
       " #   Column                    Non-Null Count  Dtype \n",
       "---  ------                    --------------  ----- \n",
       " 0   Record_Number             63388 non-null  int64 \n",
       " 1   Region                    63388 non-null  object\n",
       " 2   RESIDENCE_TYPE            63388 non-null  object\n",
       " 3   Family_Composition        63388 non-null  object\n",
       " 4   sex                       63388 non-null  int64 \n",
       " 5   age                       63388 non-null  int64 \n",
       " 6   Marital_Status            63388 non-null  int64 \n",
       " 7   student                   63388 non-null  int64 \n",
       " 8   Country_Of_Birth          63388 non-null  int64 \n",
       " 9   health                    63388 non-null  int64 \n",
       " 10  Ethnic_Group              63388 non-null  int64 \n",
       " 11  religion                  63388 non-null  int64 \n",
       " 12  Economic_Activity         63388 non-null  object\n",
       " 13  Occupation                63388 non-null  object\n",
       " 14  industry                  63388 non-null  object\n",
       " 15  Hours_Worked_Per_Week     63388 non-null  object\n",
       " 16  Approximate_Social_Grade  63388 non-null  object\n",
       "dtypes: int64(9), object(8)\n",
       "memory usage: 8.2+ MB\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490442e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Record_Number: int64\n",
       "Region: object\n",
       "RESIDENCE_TYPE: object\n",
       "Family_Composition: object\n",
       "sex: int64\n",
       "age: int64\n",
       "Marital_Status: int64\n",
       "student: int64\n",
       "Country_Of_Birth: int64\n",
       "health: int64\n",
       "Ethnic_Group: int64\n",
       "religion: int64\n",
       "Economic_Activity: object\n",
       "Occupation: object\n",
       "industry: object\n",
       "Hours_Worked_Per_Week: object\n",
       "Approximate_Social_Grade: object\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for column, dtype in df.dtypes.items():\n",
    "    print(f\"{column}: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2ebd34-daea-47eb-aa72-914eb174c9fb",
   "metadata": {},
   "source": [
    "Next, we read the JSON file with the interpretations of categories. And print all the types of the JSON dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2455b3ef-fb85-45f4-80f2-3871170d9483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'str'>\n",
       "<class 'str'>\n",
       "<class 'str'>\n",
       "<class 'str'>\n",
       "<class 'str'>\n",
       "<class 'str'>\n",
       "<class 'str'>\n",
       "<class 'str'>\n",
       "<class 'str'>\n",
       "<class 'str'>\n",
       "<class 'str'>\n",
       "<class 'str'>\n",
       "<class 'str'>\n",
       "<class 'str'>\n",
       "<class 'str'>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"./data/data_dictionary.json\", \"r\") as read_file:\n",
    "    labels = json.load(read_file)\n",
    "\n",
    "for i in labels:\n",
    "    print(type(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dc5233",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
